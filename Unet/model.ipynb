{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "# import cv2\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "# from cv2 import imread, imshow, waitKey, destroyAllWindows, IMREAD_GRAYSCALE\n",
    "from model_structure_3d import UNet3D as UNet\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the seed for the possibility to repeat the result\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "\n",
    "# Images dimensions\n",
    "IMG_WIDTH = config.WIDTH\n",
    "IMG_HEIGHT = config.HEIGHT\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "# Define the path to data folder\n",
    "IN_DATA_PATH = \"./dataset/indata/\"\n",
    "OUT_DATA_PATH = \"./dataset/outdata/\"\n",
    "\n",
    "# Define the ratio of data to be used for training (e.g., 80%)\n",
    "TRAIN_RATIO = 0.75\n",
    "\n",
    "# List all items (files) in the data folder\n",
    "all_items_X = os.listdir(IN_DATA_PATH)\n",
    "l = len(all_items_X)\n",
    "all_items_X = all_items_X\n",
    "all_items_Y = os.listdir(OUT_DATA_PATH)\n",
    "\n",
    "# Shuffle the list of items randomly, without losing the connection between \n",
    "# Function for grouping the elements\n",
    "def group_by_n(lst, n):\n",
    "    return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
    "all_items_X = group_by_n(all_items_X, config.NUM_PICS)[:-1]\n",
    "all_items_Y = group_by_n(all_items_Y, config.NUM_PICS)[:-1]\n",
    "data = list(zip(all_items_X, all_items_Y))\n",
    "\n",
    "np.random.shuffle(data)\n",
    "all_items_X, all_items_Y = zip(*data)\n",
    "all_items_X = np.array(all_items_X)[0:l // 2]\n",
    "all_items_Y = np.array(all_items_Y)[0:l // 2]\n",
    "\n",
    "# Calculate the split index based on the training ratio\n",
    "split_index = int(len(all_items_X) * TRAIN_RATIO)\n",
    "\n",
    "# Split the items into training and testing sets\n",
    "train_items_X = all_items_X[:split_index]\n",
    "test_items_X = all_items_X[split_index:]\n",
    "\n",
    "train_items_Y = all_items_Y[:split_index]\n",
    "test_items_Y = all_items_Y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the arrays to store training and testing data\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "# Load the data\n",
    "for array in tqdm(train_items_X, desc=\"Loading Training Images\"):\n",
    "    X_train_temp = []\n",
    "    Y_train_temp = []\n",
    "    for item in array:\n",
    "        path = os.path.join(IN_DATA_PATH, item)\n",
    "        img = plt.imread(path)\n",
    "        X_train_temp.append(img / 255)\n",
    "        Y_train_temp.append((img >= 61).astype(np.uint8))\n",
    "    X_train.append(X_train_temp)\n",
    "    Y_train.append(Y_train_temp)\n",
    "\n",
    "for item in tqdm(test_items_X, desc=\"Loading Test Images\"):\n",
    "    X_test_temp = []\n",
    "    Y_test_temp = []\n",
    "    for item in array:\n",
    "        path = os.path.join(IN_DATA_PATH, item)\n",
    "        img = plt.imread(path)\n",
    "        X_test_temp.append(img / 255)\n",
    "        Y_test_temp.append((img >= 61).astype(np.uint8))\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# b = random.randint(0, len(X_train))\n",
    "# image_index = b\n",
    "# imshow(X_train[image_index])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[image_index]))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model\n",
    "model = UNet().to(device)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n",
    "\n",
    "# Create the model\n",
    "model = UNet()\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Criterion and optimizer created.\")\n",
    "\n",
    "# Add a channel dimension (1 channel)\n",
    "X_train_tensor = torch.Tensor(X_train).unsqueeze(1).to(device)  \n",
    "Y_train_tensor = torch.Tensor(Y_train).unsqueeze(1).to(device)\n",
    "print(\"Unsqueezed.\")\n",
    "\n",
    "# Create a DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "print(\"Loaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1, 11]\n",
    "# Training loop\n",
    "epochs = int(input(\"Enter preffered epoch count: \"))\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = (loss * (weights[0] + labels * (weights[1] - weights[0]))).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "# Saving the model's state dictionary\n",
    "torch.save(deepcopy(model).cpu().state_dict(), 'model_for_vasc.pth')\n",
    "\n",
    "# Validation and prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "preds_train = []\n",
    "preds_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds_train.append(outputs.cpu().numpy())\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    preds_test.append(outputs.cpu().numpy())\n",
    "\n",
    "# Convert predictions to binary masks\n",
    "preds_train = (np.concatenate(preds_train) > 0.5).astype(np.uint8)\n",
    "preds_test = (np.concatenate(preds_test) > 0.5).astype(np.uint8)\n",
    "\n",
    "# Perform a sanity check on random training samples\n",
    "# ix = random.randint(0, len(preds_test))\n",
    "# plt.imshow(X_test[ix].squeeze(), cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(Y_test[ix].squeeze(), cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(preds_test[ix].squeeze(), cmap='gray')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to binary masks\n",
    "preds_test_binary = (preds_test > 0.5).astype(np.uint8)\n",
    "\n",
    "# Flatten the ground truth masks (Y_test) and predicted masks (preds_test_binary)\n",
    "y_true = Y_test.flatten()\n",
    "y_pred = preds_test_binary.flatten()\n",
    "\n",
    "# Calculate TP, FP, TN, FN\n",
    "TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "# Display TP, FP, TN, FN\n",
    "print(\"True Positives (TP):\", TP)\n",
    "print(\"False Positives (FP):\", FP)\n",
    "print(\"True Negatives (TN):\", TN)\n",
    "print(\"False Negatives (FN):\", FN)\n",
    "\n",
    "# Calculate other parameters\n",
    "print(\"Positive's accuracy:\", TP / (TP + FN))\n",
    "print(\"Negative's accuracy:\", TN / (TN + FP))\n",
    "\n",
    "print(\"Positive's recall:\", TP / (TP + FP))\n",
    "print(\"Negative's recaLL:\", TN / (TN + FN))\n",
    "\n",
    "print(\"Total accuracy:\", (TP + TN) / (TP + TN + FP + FN))\n",
    "print(\"Weighted accuracy (multiplier = 4):\", (4 * TP + TN) / (4 * TP + TN + FP + 4 * FN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
