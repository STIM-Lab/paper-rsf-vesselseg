{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import config\n",
    "\n",
    "from model_structure import UNet\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PyTorch model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load(\"./model_for_vasc.pth\", map_location=\"cpu\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Directory paths for input and output data\n",
    "input_dir = \"./preparations/preprocess/indata\"\n",
    "output_dir = \"./preparations/preprocess/outdata\"\n",
    "\n",
    "# Get a list of image files in the input directory\n",
    "image_files = os.listdir(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atashitk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 18\n",
      "TN: 4055\n",
      "FP: 5\n",
      "FN: 18 \n",
      "\n",
      "Positive accuracy: 0.5\n",
      "Negative accuracy: 0.999\n",
      "Accuracy: 0.994\n",
      "\n",
      "Positive Recall: 0.783\n",
      "Negative Recall: 0.996\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Choose a random pair of input and output files\n",
    "    random_image_file = random.choice(image_files)\n",
    "\n",
    "    # Construct the full file paths\n",
    "    input_image_path = os.path.join(input_dir, random_image_file)\n",
    "    output_label_path = os.path.join(output_dir, random_image_file)\n",
    "    \n",
    "    label_image = cv2.imread(output_label_path, cv2.IMREAD_GRAYSCALE)\n",
    "    shape = label_image.shape\n",
    "    random_x, random_y = random.randint(0, shape[0] - config.WIDTH - 1), random.randint(0, shape[1] - config.HEIGHT - 1)\n",
    "    label_image = label_image[random_y : random_y + config.HEIGHT, random_x : random_x + config.WIDTH]\n",
    "    if cv2.countNonZero(label_image) == 0:\n",
    "        continue\n",
    "        pass\n",
    "    img = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)[random_y : random_y + config.HEIGHT, random_x : random_x + config.WIDTH]\n",
    "    mask = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,2)\n",
    "    input_image = img * (mask // 255)\n",
    "\n",
    "    shape = input_image.shape\n",
    "    break\n",
    "\n",
    "# Perform inference using the model\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(input_image / 255).unsqueeze(0).float()\n",
    "    output = model(input_tensor)\n",
    "# Convert the output tensor to a NumPy array\n",
    "output_np = output.numpy()\n",
    "# Convert the output prediction to binary format and multiply by 255\n",
    "binary_output = (output_np >= 0.5).astype(np.uint8)[0] * 255\n",
    "# Display the input image, label, and binary prediction using OpenCV\n",
    "cv2.imshow(\"Input Image\", cv2.resize(input_image, (512, 512)))\n",
    "cv2.imshow(\"Label Image\", cv2.resize(label_image, (512, 512)))\n",
    "cv2.imshow(\"Model Prediction (Binary)\", cv2.resize(binary_output, (512, 512)))\n",
    "cv2.imshow(\"Correctness Image\", cv2.resize((np.multiply(binary_output, label_image) + np.multiply((255 - binary_output), (255 - label_image))) * 255, (512, 512)))\n",
    "\n",
    "TP = np.sum(np.multiply(binary_output, label_image))\n",
    "TN = np.sum(np.multiply((255 - binary_output), (255 - label_image)))\n",
    "FP = np.sum(np.multiply(binary_output, (255 - label_image)))\n",
    "FN = np.sum(np.multiply((255 - binary_output), label_image))\n",
    "print(\"TP:\", TP)\n",
    "print(\"TN:\", TN)\n",
    "print(\"FP:\", FP)\n",
    "print(\"FN:\", FN, \"\\n\")\n",
    "\n",
    "print(\"Positive accuracy:\", np.round(TP / (TP + FN), 3))\n",
    "print(\"Negative accuracy:\", np.round(TN / (TN + FP), 3))\n",
    "print(\"Accuracy:\", np.round((TP + TN) / (TP + TN + FP + FN), 3))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Positive Recall:\", np.round(TP / (TP + FP), 3))\n",
    "print(\"Negative Recall:\", np.round(TN / (TN + FN), 3))\n",
    "\n",
    "# Wait for a key press and then close the OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img, margin):\n",
    "    if margin != 0:\n",
    "        raise ValueError\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.from_numpy(img / 255).unsqueeze(0).float()\n",
    "        output = model(input_tensor)\n",
    "    output_np = output.numpy()\n",
    "    return output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image name: img_1000958.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 42/42 [00:11<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0.13 %\n",
      "TN: 99.76 %\n",
      "FP: 0.02 %\n",
      "FN: 0.09 %\n",
      "\n",
      "Positive accuracy: 0.6023511372348581\n",
      "Negative accuracy: 0.9998103778747574\n",
      "Total accuracy: 0.9989499308437068\n",
      "\n",
      "Positive Recall: 0.8732864023712487\n",
      "Negative Recall: 0.9991378550876246\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./preparations/data/indata\"\n",
    "output_dir = \"./preparations/data/outdata\"\n",
    "\n",
    "margin = 0\n",
    "step = 32\n",
    "# Get a list of image files in the input directory\n",
    "image_files = os.listdir(input_dir)\n",
    "\n",
    "show_size = (512, 512)\n",
    "\n",
    "random_image_file = random.choice(image_files)\n",
    "print(\"Random image name:\", random_image_file)\n",
    "# random_image_file = \"img_1000342.png\"\n",
    "input_image_path = os.path.join(input_dir, random_image_file)\n",
    "output_label_path = os.path.join(output_dir, random_image_file)\n",
    "\n",
    "input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "_, label_image = cv2.threshold(input_image, 61, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "shape = label_image.shape\n",
    "\n",
    "input_image = input_image[shape[0] % step:, shape[1]%step:]\n",
    "label_image = label_image[shape[0] % step:, shape[1]%step:]\n",
    "\n",
    "shape = label_image.shape\n",
    "count_times = np.zeros(shape)\n",
    "total_times = np.zeros(shape)\n",
    "\n",
    "arr1 = []\n",
    "arr2 = []\n",
    "for i in tqdm(range(0, shape[0] - shape[0] % step, step), \"Processing\"):\n",
    "    for j in range(0, shape[1] - shape[1] % step, step):\n",
    "        slice_tmp = input_image[i : i + config.HEIGHT, j : j + config.WIDTH]\n",
    "        array_tmp = get_prediction(slice_tmp, margin)[0]\n",
    "        count_times[i : (i + config.HEIGHT), j : (j + config.WIDTH)] += (array_tmp > 0.5)\n",
    "        total_times[i : (i + config.HEIGHT), j : (j + config.WIDTH)] += 1\n",
    "      \n",
    "\n",
    "confidence = (count_times + 0.0001) / (total_times + 0.0001)\n",
    "binary_output = (np.floor(confidence * 255)).astype(np.uint8)\n",
    "\n",
    "# Create a new array with default values (127, 127, 255)\n",
    "result_array = np.full((label_image.shape[0], label_image.shape[1], 3), (0, 0, 0), dtype=np.uint8)\n",
    "\n",
    "# Update values based on conditions\n",
    "result_array[(label_image == 0) & (binary_output == 0), :] = (0, 0, 0)  # Where both are 0\n",
    "result_array[(label_image == 255) & (binary_output == 255), :] = (255, 255, 255)  # Where both are 255\n",
    "result_array[(label_image == 0) & (binary_output == 255), :] = (0, 0, 255)  # Where label_image is 0 and binary_output is 255\n",
    "result_array[(label_image == 255) & (binary_output == 0), :] = (0, 128, 128)  # Where label_image is 255 and binary_output is 0\n",
    "\n",
    "a, b, c = result_array.shape\n",
    "\n",
    "# Calculate the number of rows and columns to keep\n",
    "new_a = a - (a % step)\n",
    "new_b = b - (b % step)\n",
    "\n",
    "# Create copies for later\n",
    "label_image_copy = label_image[:new_a, :new_b]\n",
    "binary_output_copy = binary_output[:new_a, :new_b]\n",
    "# Use array slicing to obtain the desired subarray\n",
    "coef = 0.5\n",
    "\n",
    "input_image = cv2.cvtColor(input_image[:new_a, :new_b], cv2.COLOR_GRAY2RGB)\n",
    "label_image = cv2.cvtColor(label_image[:new_a, :new_b], cv2.COLOR_GRAY2RGB)\n",
    "binary_output = cv2.cvtColor(binary_output[:new_a, :new_b], cv2.COLOR_GRAY2RGB)\n",
    "result_array = result_array[:new_a, :new_b, :]\n",
    "\n",
    "modified_label_image = label_image.copy()\n",
    "COLOR_BG = [1, 1, 1] #BGR WHITE\n",
    "COLOR_BODY = [0, 0, 0] #BGR BLUE\n",
    "\n",
    "modified_label_image[np.all(label_image == [0, 0, 0], axis=-1)] = COLOR_BG\n",
    "modified_label_image[np.all(label_image == [255, 255, 255], axis=-1)] = COLOR_BODY\n",
    "\n",
    "blend_image = input_image * modified_label_image\n",
    "\n",
    "img_vert1 = np.concatenate([cv2.resize(input_image, show_size), cv2.resize(label_image, show_size)], axis=0)\n",
    "img_vert2 = np.concatenate([cv2.resize(blend_image, show_size), cv2.resize(binary_output, show_size)], axis=0).astype(np.uint8)\n",
    "\n",
    "img_collage = np.concatenate([img_vert1, img_vert2], axis=1)\n",
    "\n",
    "# Specify the text, font, and position\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_thickness = 2\n",
    "text_color = (255, 255, 255)  # White color\n",
    "size_x = input_image.shape[0] // 2\n",
    "size_y = input_image.shape[1] // 2\n",
    "text_position_x = 10\n",
    "text_position_y = 25\n",
    "\n",
    "# Use cv2.putText() to overlay text on the image\n",
    "cv2.putText(img_collage, \"Input\", (text_position_x, text_position_y), font, font_scale, text_color, font_thickness)\n",
    "cv2.putText(img_collage, \"Label\", (text_position_x, text_position_y + show_size[1]), font, font_scale, text_color, font_thickness)\n",
    "cv2.putText(img_collage, \"Prediction\", (text_position_x + show_size[0], text_position_y  + show_size[1]), font, font_scale, text_color, font_thickness)\n",
    "cv2.putText(img_collage, \"Blend\", (text_position_x + show_size[0], text_position_y), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "cv2.imshow(\"Input/Output - Label/LabelCorr\", img_collage)\n",
    "cv2.imwrite(\"./gener_image.png\", result_array)\n",
    "\n",
    "TP = np.count_nonzero(np.multiply(binary_output_copy, label_image_copy))\n",
    "TN = np.count_nonzero(np.multiply((255 - binary_output_copy), (255 - label_image_copy)))\n",
    "FP = np.count_nonzero(np.multiply(binary_output_copy, (255 - label_image_copy)))\n",
    "FN = np.count_nonzero(np.multiply((255 - binary_output_copy), label_image_copy))\n",
    "print(\"TP:\", np.round(TP / (TP + TN + FP + FN) * 100, 2), \"%\")\n",
    "print(\"TN:\", np.round(TN / (TP + TN + FP + FN) * 100, 2), \"%\")\n",
    "print(\"FP:\", np.round(FP / (TP + TN + FP + FN) * 100, 2), \"%\")\n",
    "print(\"FN:\", np.round(FN / (TP + TN + FP + FN) * 100, 2), \"%\")\n",
    "\n",
    "print()\n",
    "print(\"Positive accuracy:\", TP / (TP + FN))\n",
    "print(\"Negative accuracy:\", TN / (TN + FP))\n",
    "print(\"Total accuracy:\", (TP + TN) / (TP + TN + FP + FN))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Positive Recall:\", TP / (TP + FP))\n",
    "print(\"Negative Recall:\", TN / (TN + FN))\n",
    "\n",
    "# Wait for a key press and then close the OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.putText(result_array, \"Yellow - FN\", (text_position_x, text_position_y), font, font_scale, text_color, font_thickness)\n",
    "cv2.putText(result_array, \"Red - FP\", (text_position_x, 2 * text_position_y), font, font_scale, text_color, font_thickness)\n",
    "cv2.imshow(\"Correctness Image\", result_array)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = \"./preparations/data/indata\"\n",
    "# output_dir = \"./preparations/preprocess/\"\n",
    "# # Create the folder if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# os.makedirs(os.path.join(output_dir, \"indata\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(output_dir, \"outdata\"), exist_ok=True)\n",
    "\n",
    "# for image_file in tqdm(os.listdir(input_dir)):\n",
    "#     input_image_path = os.path.join(input_dir, image_file)\n",
    "#     output_image_path1 = os.path.join(output_dir, \"indata\", image_file)\n",
    "#     output_image_path2 = os.path.join(output_dir, \"outdata\", image_file)\n",
    "#     img = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     mask = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "#     output_image = img * (mask // 255)\n",
    "#     cv2.imwrite(output_image_path1, output_image)\n",
    "#     _, thresh = cv2.threshold(output_image, 61, 255, cv2.THRESH_BINARY)\n",
    "#     cv2.imwrite(output_image_path2, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def nii_to_numpy(file_path):\n",
    "#     # Load NRRD file\n",
    "#     nrrd_data = nib.load(file_path)\n",
    "\n",
    "#     # Get the data array from the NRRD file\n",
    "#     nrrd_array = nrrd_data.get_fdata()\n",
    "\n",
    "#     return nrrd_array\n",
    "\n",
    "# # Example usage\n",
    "# nrrd_file_path = \"./single_image.nii\"\n",
    "# numpy_array = np.squeeze(nii_to_numpy(nrrd_file_path), axis=-1)\n",
    "\n",
    "# img_name = \"img_1000342.png\"\n",
    "# img1 = cv2.imread(os.path.join(\"./preparations/data/indata/\", img_name), cv2.IMREAD_GRAYSCALE)\n",
    "# img2 = cv2.imread(os.path.join(\"./preparations/data/outdata\", img_name), cv2.IMREAD_GRAYSCALE)\n",
    "# img3 = np.transpose(numpy_array * 255)\n",
    "\n",
    "# size = (512, 512)\n",
    "# min_err = 2e-10\n",
    "# min_err_threshold = 256\n",
    "\n",
    "# weight = 100\n",
    "\n",
    "\n",
    "# _, img_threshold = cv2.threshold(img1, 61, 255, cv2.THRESH_BINARY)\n",
    "# cv2.imshow(\"Original\", cv2.resize(img1, size))\n",
    "# cv2.imshow(\"Threshold\", cv2.resize(img2, size))\n",
    "# cv2.imshow(\"Manual\", cv2.resize(img3, size))\n",
    "# cv2.imshow(\"Closest\", cv2.resize(img_threshold, size))\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Create a new array with default values (255, 0, 0)\n",
    "# result_array = np.full((img3.shape[0], img3.shape[1], 3), (0, 0, 255), dtype=np.uint8)\n",
    "\n",
    "# # Update values based on conditions\n",
    "# result_array[(img3 == 0) & (img_threshold == 0), :] = (0, 0, 0)  # Where both are 0\n",
    "# result_array[(img3 == 255) & (img_threshold == 255), :] = (255, 255, 255)  # Where both are 255\n",
    "\n",
    "\n",
    "# cv2.imshow(\"Errors\", result_array)\n",
    "# cv2.imwrite(\"./image_with_errors.png\", result_array)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1344, 1344)\n",
      "(1356, 1356)\n",
      "(1344, 1344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1340/1341 [00:45<00:00, 29.40it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ://Artem/annotated_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\n\u001b[0;32m     23\u001b[0m pic \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m---> 24\u001b[0m height, width \u001b[38;5;241m=\u001b[39m \u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     25\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(file_path, pic[:height \u001b[38;5;241m-\u001b[39m height \u001b[38;5;241m%\u001b[39m step, :width \u001b[38;5;241m-\u001b[39m width \u001b[38;5;241m%\u001b[39m step])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "orig_data = os.listdir(\"Z://Artem/orig_data/\")\n",
    "annotated_data = os.listdir(\"Z://Artem/annotated_data/\")\n",
    "pic_in = cv2.imread(\"Z://Artem/orig_data/img_1000000.png\", cv2.IMREAD_GRAYSCALE)\n",
    "pic_out = cv2.imread(\"Z://Artem/annotated_data/img_1000000.png\", cv2.IMREAD_GRAYSCALE)\n",
    "pic_pred = cv2.imread(\"Z://Artem/CNN_data/img_1000000.png\", cv2.IMREAD_GRAYSCALE)\n",
    "print(pic_in.shape)\n",
    "print(pic_out.shape)\n",
    "print(pic_pred.shape)\n",
    "\n",
    "orig_data = os.listdir(\"Z://Artem/orig_data/\")\n",
    "annotated_data = os.listdir(\"Z://Artem/annotated_data/\")\n",
    "\n",
    "step = 32\n",
    "\n",
    "# for x in tqdm(orig_data, \"\"):\n",
    "#     file_path = os.path.join(\"Z://Artem/orig_data/\", x)\n",
    "#     pic = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     height, width = pic.shape\n",
    "#     cv2.imwrite(file_path, pic[:height- height % step, :width - width % step])\n",
    "\n",
    "for x in tqdm(annotated_data, \"\"):\n",
    "    file_path = os.path.join(\"Z://Artem/annotated_data/\", x)\n",
    "    pic = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    height, width = pic.shape\n",
    "    cv2.imwrite(file_path, pic[:height - height % step, :width - width % step])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
